{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement logistic regression to classify the images provided in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appying the principal component analysis to get the transformed trainning set where the number of principal component is 74. Returnning the transformed traning set and the resultant eigenvectors which will be used to tranformed the test dataset also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(train):\n",
    "    n_components = 74\n",
    "    x_mean = np.mean(train, axis =0)\n",
    "    x_center = train - x_mean\n",
    "    x_cov = np.cov(x_center.T)\n",
    "    x_eigenvalues, x_eigenvectors = np.linalg.eig(x_cov)\n",
    "    indexes = x_eigenvalues.argsort()[::-1]   \n",
    "    eigenvalues = x_eigenvalues[indexes]\n",
    "    eigenvectors = x_eigenvectors[:,indexes]\n",
    "    red_eigenvec = eigenvectors[:,:n_components]\n",
    "    x_pca = red_eigenvec.T.dot(train.T)\n",
    "    global_eigenvector = eigenvectors\n",
    "    return x_pca.T,eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the test data set by applying the principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pca(eigenvectors, test):\n",
    "    n_components = 74\n",
    "    red_eigenvec = eigenvectors[:,:n_components]\n",
    "    test_pca = red_eigenvec.T.dot(test.T)\n",
    "    return test_pca.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y):\n",
    "    return ((predictions == y).mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function \n",
    "\\begin{align}\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{\\theta^{\\top} x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(x):\n",
    "    g = 1/(1 + np.exp(-x))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of the cost:\n",
    "\\begin{align}\n",
    "J(\\theta) & = \\dfrac{1}{m} \\sum_{i=1}^m \\mathrm{Cost}(h_\\theta(x^{(i)}),y^{(i)}) \\\\\n",
    "& = - \\dfrac{1}{m} [\\sum_{i=1}^{m} y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1-h_\\theta(x^{(i)}))] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(x,y,w,h):\n",
    "    total_cost = np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h))\n",
    "    cost = total_cost/len(y)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calulation of the gradient\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\dfrac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claculate_gradient(x,y,h):\n",
    "    gradient = (y-h).dot(x)/len(y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning the data :-  One vs Rest classification technique is used. One-vs-Rest classification is a method which involves training N distinct binary classifiers, each designed for recognizing a particular class. Then those N classifiers are collectively used for multi-class classification. I take values of one class and turn them into one, and the rest of classes - into zeros. And everytime cosidering only one class, it converges by running through the number of iterations and calculate the optimized weight corrrsponding to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(xtrain,y,numofiter,learningrate):\n",
    "    x = np.ones(shape=(xtrain.shape[0], xtrain.shape[1] + 1),dtype=complex)\n",
    "    x[:, 1:] = xtrain\n",
    "    classes = np.unique(y)\n",
    "    weights =[]\n",
    "    for cls in classes:\n",
    "        weight = np.zeros(x.shape[1],dtype=complex) #dtype=complex\n",
    "        y_map = []\n",
    "        for i in y:\n",
    "            if(i==cls):\n",
    "                y_map.append(1)\n",
    "            else:\n",
    "                y_map.append(0)\n",
    "        #print(\"ymap \",y_map)\n",
    "        for i in range(numofiter):\n",
    "            h = sigmoid_function(x.dot(weight))\n",
    "            gradient = claculate_gradient(x,y_map,h)\n",
    "            weight = weight + learningrate*gradient\n",
    "        weights.append(weight)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict class: Taking each sample  and calculating\n",
    "\\begin{align}\n",
    "h_{\\theta}(x) = [h_{\\theta}^{(1)}(x), h_{\\theta}^{(2)}(x), h_{\\theta}^{(3)}(x),.........]\n",
    "\\end{align}\n",
    "# and taking the maximum value and its corresponding class as the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(xval,y,weights):\n",
    "    x = np.ones(shape=(xval.shape[0], xval.shape[1] + 1),dtype=complex)\n",
    "    x[:, 1:] = xval\n",
    "    temp_predictions = []\n",
    "    for i in x:\n",
    "        hypothesis = np.zeros(shape=(len(weights)),dtype=complex) #dtype=complex\n",
    "        k =0\n",
    "        for weight in weights:\n",
    "            h = sigmoid_function(i.dot(weight))\n",
    "            #print(\"h===== \",h)\n",
    "            hypothesis[k] = h\n",
    "            k = k+1\n",
    "        temp_predictions.append(np.argmax(hypothesis))\n",
    "    predictions = []\n",
    "    #print(\"t_pred \",temp_predictions)\n",
    "    for indx in temp_predictions:\n",
    "        #print(\"y_indx\",indx)\n",
    "        predictions.append(y[indx])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the tranning data and applying the own PCA on it taking 74 principal components and applying min-max scaler over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"G:\\second_sem\\SMAI\\Assignment_3\\q2\\dataset\"\n",
    "images = []\n",
    "labels = []\n",
    "for f in os.listdir(path):\n",
    "    label = f[1:f.find(\"_\")]\n",
    "    images.append(np.asarray(Image.open(path +'/'+f).convert('L').resize((64, 64))).flatten())\n",
    "    labels.append(label)\n",
    "train = np.array(images)\n",
    "scalar = MinMaxScaler()\n",
    "train = scalar.fit_transform(train)\n",
    "train,eigenvects = apply_pca(train)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = train[:,:]\n",
    "ytrain = labels[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the test data and transforming the test data by applying the own PCA on it taking 74 principal components and applying min-max scaler over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpath = r\"G:\\second_sem\\SMAI\\Assignment_3\\q2\\A3\\test\"\n",
    "vimages = []\n",
    "vlabels = []\n",
    "for f in os.listdir(vpath):\n",
    "    label = f[1:f.find(\"_\")]\n",
    "    vimages.append(np.asarray(Image.open(vpath +'/'+f).convert('L').resize( (64, 64))).flatten())\n",
    "    vlabels.append(label)\n",
    "vtrain = np.array(vimages)\n",
    "vtrain = scalar.transform(vtrain)\n",
    "vtrain = transform_pca(eigenvects,vtrain)\n",
    "vlabels = np.array(vlabels)\n",
    "xvalidation = vtrain[:,:]\n",
    "yvalidation = vlabels[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1:- The number of iterations = 100000 and the learning rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "numofiter = 100000\n",
    "learningrate = 0.0001\n",
    "weights = train_data(xtrain, ytrain, numofiter,learningrate)\n",
    "classes = np.unique(ytrain)\n",
    "predictions1 = predict_class(xvalidation,classes,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.75"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc1 = accuracy(predictions1, yvalidation)\n",
    "acc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  0  0  0  0]\n",
      " [ 0 13  0  0  0  0]\n",
      " [ 0  3  9  0  0  0]\n",
      " [ 0  1  2  0  0  0]\n",
      " [ 0  0  1  0  0  0]\n",
      " [ 0  2  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(predictions1,yvalidation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          01       0.00      0.00      0.00         1\n",
      "          03       0.65      1.00      0.79        13\n",
      "          04       0.75      0.75      0.75        12\n",
      "          05       0.00      0.00      0.00         3\n",
      "          06       0.00      0.00      0.00         1\n",
      "          07       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.69        32\n",
      "   macro avg       0.23      0.29      0.26        32\n",
      "weighted avg       0.55      0.69      0.60        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Indranil\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions1,yvalidation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2:- The number of iterations = 50000 and the learning rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "numofiter = 50000\n",
    "learningrate = 0.0005\n",
    "weights = train_data(xtrain, ytrain, numofiter,learningrate)\n",
    "classes = np.unique(ytrain)\n",
    "predictions2 = predict_class(xvalidation,classes,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.125"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc2 = accuracy(predictions2, yvalidation)\n",
    "acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0]\n",
      " [ 0  0 15  0  0  0]\n",
      " [ 0  0  2 10  0  0]\n",
      " [ 0  0  0  1  0  0]\n",
      " [ 0  0  2  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(predictions2,yvalidation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          00       0.00      0.00      0.00         1\n",
      "          02       0.00      0.00      0.00         1\n",
      "          03       0.75      1.00      0.86        15\n",
      "          04       0.83      0.83      0.83        12\n",
      "          05       0.00      0.00      0.00         1\n",
      "          07       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.78        32\n",
      "   macro avg       0.26      0.31      0.28        32\n",
      "weighted avg       0.66      0.78      0.71        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions2,yvalidation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3:- The number of iterations = 70000 and the learning rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "numofiter = 70000\n",
    "learningrate = 0.005\n",
    "weights = train_data(xtrain, ytrain, numofiter,learningrate)\n",
    "classes = np.unique(ytrain)\n",
    "predictions3 = predict_class(xvalidation,classes,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.375"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc3 = accuracy(predictions3, yvalidation)\n",
    "acc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0  0]\n",
      " [ 2 11  0  0]\n",
      " [ 0  1  0  0]\n",
      " [ 2  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(predictions3,yvalidation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          03       0.80      1.00      0.89        16\n",
      "          04       0.92      0.85      0.88        13\n",
      "          06       0.00      0.00      0.00         1\n",
      "          07       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.43      0.46      0.44        32\n",
      "weighted avg       0.77      0.84      0.80        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions3,yvalidation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4:- The number of iterations = 100000 and the learning rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "numofiter = 100000\n",
    "learningrate = 0.01\n",
    "weights = train_data(xtrain, ytrain, numofiter,learningrate)\n",
    "classes = np.unique(ytrain)\n",
    "predictions4 = predict_class(xvalidation,classes,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.5"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc4 = accuracy(predictions4, yvalidation)\n",
    "acc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  0  0  0]\n",
      " [ 1 11  0  0]\n",
      " [ 0  1  0  0]\n",
      " [ 2  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(predictions4,yvalidation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samle Test And Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"G:\\second_sem\\SMAI\\Assignment_3\\q2\\sample_train.txt\"\n",
    "train_images = []\n",
    "train_labels = []\n",
    "train_file = open(train_path,\"r\")\n",
    "for train_line in train_file:\n",
    "    path_label = train_line.split(\" \")\n",
    "    train_f = path_label[0]\n",
    "    train_label = path_label[1].replace('\\n', '')\n",
    "    train_images.append(np.asarray(Image.open(train_f).convert('L').resize((64, 64))).flatten())\n",
    "    train_labels.append(train_label)\n",
    "train_train = np.array(train_images)\n",
    "train_scalar = MinMaxScaler()\n",
    "train_train = train_scalar.fit_transform(train_train)\n",
    "train_train,train_eigenvects = apply_pca(train_train)\n",
    "train_labels = np.array(train_labels)\n",
    "train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xtrain = train_train[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ordinal = {}\n",
    "ordinal_to_label = {}\n",
    "uniq_label = np.unique(train_labels)\n",
    "for i in range(uniq_label.shape[0]):\n",
    "    label_to_ordinal[uniq_label[i]] = i\n",
    "for j in range(uniq_label.shape[0]):\n",
    "    ordinal_to_label[j] = uniq_label[j]\n",
    "train_ytrain = np.zeros(len(train_labels))\n",
    "for i in range(len(train_labels)):\n",
    "    train_ytrain[i] = label_to_ordinal[train_labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = r\"G:\\second_sem\\SMAI\\Assignment_3\\q2\\sample_test.txt\"\n",
    "test_images = []\n",
    "test_file = open(test_path,\"r\")\n",
    "for test_line in test_file:\n",
    "    test_line= test_line.replace('\\n','')\n",
    "    test_images.append(np.asarray(Image.open(test_line).convert('L').resize( (64, 64))).flatten())\n",
    "test_test = np.array(test_images)\n",
    "test_test = train_scalar.transform(test_test)\n",
    "test_test = transform_pca(train_eigenvects,test_test)\n",
    "test_xtest = test_test[:,:]\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numofiter = 100000\n",
    "learningrate = 0.01\n",
    "t_weights = train_data(train_xtrain, train_ytrain, numofiter,learningrate)\n",
    "t_classes = np.unique(train_ytrain)\n",
    "predictions5 = predict_class(test_xtest,t_classes,t_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'abc', 'alice', 'bob', 'bob']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "for i in predictions5:\n",
    "    predicted_labels.append(ordinal_to_label[i])\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
